{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification with Spark NLP"
      ],
      "metadata": {
        "id": "6Gzchw3biO38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q pyspark==3.4.1 spark-nlp==5.1.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrfwr_q4iSHP",
        "outputId": "6af1af23-0f60-43c3-9491-00d06e5729c4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.3/536.3 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import sparknlp\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.annotator import *\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "cT8rfHZPi3-F",
        "outputId": "5ee8ea2c-0e6e-40f2-ad2c-46f474e44187"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version:  5.1.2\n",
            "Apache Spark version:  3.4.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7bf9f23f8df0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ada33d0a5d75:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsDF = spark.read \\\n",
        "      .option(\"header\", True) \\\n",
        "      .csv(\"/content/news_category_train.csv\")\n",
        "\n",
        "\n",
        "newsDF.show(truncate=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ioe51ixxkHbL",
        "outputId": "5929fcb4-c58f-49c5-83f3-9aecd8579c25"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------------------------------------+\n",
            "|category|                                       description|\n",
            "+--------+--------------------------------------------------+\n",
            "|Business| Short sellers, Wall Street's dwindling band of...|\n",
            "|Business| Private investment firm Carlyle Group, which h...|\n",
            "|Business| Soaring crude prices plus worries about the ec...|\n",
            "|Business| Authorities have halted oil export flows from ...|\n",
            "|Business| Tearaway world oil prices, toppling records an...|\n",
            "|Business| Stocks ended slightly higher on Friday but sta...|\n",
            "|Business| Assets of the nation's retail money market mut...|\n",
            "|Business| Retail sales bounced back a bit in July, and n...|\n",
            "|Business|\" After earning a PH.D. in Sociology, Danny Baz...|\n",
            "|Business| Short sellers, Wall Street's dwindling  band o...|\n",
            "|Business| Soaring crude prices plus worries  about the e...|\n",
            "|Business| OPEC can do nothing to douse scorching  oil pr...|\n",
            "|Business| Non OPEC oil exporters should consider  increa...|\n",
            "|Business| WASHINGTON/NEW YORK (Reuters) - The auction fo...|\n",
            "|Business| The dollar tumbled broadly on Friday  after da...|\n",
            "|Business|If you think you may need to help your elderly ...|\n",
            "|Business|The purchasing power of kids is a big part of w...|\n",
            "|Business|There is little cause for celebration in the st...|\n",
            "|Business|The US trade deficit has exploded 19 to a recor...|\n",
            "|Business|Oil giant Shell could be bracing itself for a t...|\n",
            "+--------+--------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsDF.take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIRjgUAHkTSl",
        "outputId": "2d123484-5f7c-49ca-817d-ae2aaf622120"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(category='Business', description=\" Short sellers, Wall Street's dwindling band of ultra cynics, are seeing green again.\"),\n",
              " Row(category='Business', description=' Private investment firm Carlyle Group, which has a reputation for making well timed and occasionally controversial plays in the defense industry, has quietly placed its bets on another part of the market.')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "newsDF.groupBy(\"category\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIEAevaQkY5C",
        "outputId": "e49ef136-3005-407e-9ad0-a26c45398700"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|category|count|\n",
            "+--------+-----+\n",
            "|   World|30000|\n",
            "|Sci/Tech|30000|\n",
            "|  Sports|30000|\n",
            "|Business|30000|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building classification Pipeline"
      ],
      "metadata": {
        "id": "Aeav_02QkelN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LogReg with CountVectorizer\n"
      ],
      "metadata": {
        "id": "hVtpYWlwkl9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer, HashingTF, IDF, OneHotEncoder, StringIndexer, VectorAssembler, SQLTransformer"
      ],
      "metadata": {
        "id": "Cz-CaNkvkh87"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "      .setInputCol(\"description\") \\\n",
        "      .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "      .setInputCols([\"document\"]) \\\n",
        "      .setOutputCol(\"token\")\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "      .setInputCols([\"token\"]) \\\n",
        "      .setOutputCol(\"normalized\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"normalized\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\n",
        "\n",
        "stemmer = Stemmer() \\\n",
        "      .setInputCols([\"cleanTokens\"]) \\\n",
        "      .setOutputCol(\"stem\")\n",
        "\n",
        "finisher = Finisher() \\\n",
        "      .setInputCols([\"stem\"]) \\\n",
        "      .setOutputCols([\"token_features\"]) \\\n",
        "      .setOutputAsArray(True) \\\n",
        "      .setCleanAnnotations(False)\n",
        "\n",
        "countVectors = CountVectorizer(inputCol=\"token_features\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
        "\n",
        "nlp_pipeline = Pipeline(\n",
        "    stages=[document_assembler,\n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner,\n",
        "            stemmer,\n",
        "            finisher,\n",
        "            countVectors,\n",
        "            label_stringIdx])\n",
        "\n",
        "nlp_model = nlp_pipeline.fit(newsDF)\n",
        "\n",
        "processed = nlp_model.transform(newsDF)\n",
        "\n",
        "processed.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_xjA4zrk4jD",
        "outputId": "3649fad2-dc4c-451a-8ee0-a7cb9cf548b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 814 ms, sys: 90.9 ms, total: 905 ms\n",
            "Wall time: 1min 44s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed.select('description','token_features').show(truncate=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfM5sQyXlZY2",
        "outputId": "beade9e3-f6f8-4bf2-c2b5-cd17197e4004"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------+\n",
            "|    description| token_features|\n",
            "+---------------+---------------+\n",
            "| Short selle...|[short, sell...|\n",
            "| Private inv...|[privat, inv...|\n",
            "| Soaring cru...|[soar, crude...|\n",
            "| Authorities...|[author, hal...|\n",
            "| Tearaway wo...|[tearawai, w...|\n",
            "| Stocks ende...|[stock, end,...|\n",
            "| Assets of t...|[asset, nati...|\n",
            "| Retail sale...|[retail, sal...|\n",
            "|\" After earn...|[earn, phd, ...|\n",
            "| Short selle...|[short, sell...|\n",
            "| Soaring cru...|[soar, crude...|\n",
            "| OPEC can do...|[opec, noth,...|\n",
            "| Non OPEC oi...|[non, opec, ...|\n",
            "| WASHINGTON/...|[washingtonn...|\n",
            "| The dollar ...|[dollar, tum...|\n",
            "|If you think...|[think, mai,...|\n",
            "|The purchasi...|[purchas, po...|\n",
            "|There is lit...|[littl, caus...|\n",
            "|The US trade...|[u, trade, d...|\n",
            "|Oil giant Sh...|[oil, giant,...|\n",
            "+---------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed.select('token_features').take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBn3DAoXliyR",
        "outputId": "276d0b50-7195-43a3-df99-c8ee5b2f2bb2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(token_features=['short', 'seller', 'wall', 'street', 'dwindl', 'band', 'ultra', 'cynic', 'see', 'green']),\n",
              " Row(token_features=['privat', 'invest', 'firm', 'carlyl', 'group', 'reput', 'make', 'well', 'time', 'occasion', 'controversi', 'plai', 'defens', 'industri', 'quietli', 'place', 'bet', 'anoth', 'part', 'market'])]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed.select('features').take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D2en-LMlmxm",
        "outputId": "294b84f3-9adf-41df-9f87-43c6dbed45cb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(features=SparseVector(10000, {241: 1.0, 384: 1.0, 467: 1.0, 745: 1.0, 838: 1.0, 2227: 1.0, 3670: 1.0, 6142: 1.0, 6227: 1.0})),\n",
              " Row(features=SparseVector(10000, {26: 1.0, 38: 1.0, 46: 1.0, 68: 1.0, 117: 1.0, 155: 1.0, 182: 1.0, 197: 1.0, 246: 1.0, 304: 1.0, 320: 1.0, 407: 1.0, 427: 1.0, 621: 1.0, 867: 1.0, 2366: 1.0, 2825: 1.0, 2858: 1.0, 6779: 1.0}))]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed.select('description','features','label').show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQY3iBOolr_4",
        "outputId": "e49088a3-f567-4567-e1a0-5c941d4e9610"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-----+\n",
            "|         description|            features|label|\n",
            "+--------------------+--------------------+-----+\n",
            "| Short sellers, W...|(10000,[241,384,4...|  0.0|\n",
            "| Private investme...|(10000,[26,38,46,...|  0.0|\n",
            "| Soaring crude pr...|(10000,[15,28,46,...|  0.0|\n",
            "| Authorities have...|(10000,[0,32,35,4...|  0.0|\n",
            "| Tearaway world o...|(10000,[1,2,11,28...|  0.0|\n",
            "+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed for reproducibility\n",
        "(trainingData, testData) = processed.randomSplit([0.7, 0.3], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2YfR8filrzl",
        "outputId": "911ba485-cc7c-47d8-a367-1055cb36aa1f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Count: 84003\n",
            "Test Dataset Count: 35997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainingData.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OJtIUVymwPx",
        "outputId": "094495e4-12f3-4fa5-aee4-c75e85485471"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- category: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- document: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- token: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- normalized: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- cleanTokens: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- stem: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- token_features: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- label: double (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0)\n",
        "\n",
        "lrModel = lr.fit(trainingData)\n",
        "\n",
        "predictions = lrModel.transform(testData)\n",
        "\n",
        "predictions.filter(predictions['prediction'] == 0) \\\n",
        "    .select(\"description\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmAGfNdXnCBK",
        "outputId": "f5177f03-c6ba-4e05-a9e1-70be4c3ae427"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------+---------------+-----+----------+\n",
            "|    description|category|    probability|label|prediction|\n",
            "+---------------+--------+---------------+-----+----------+\n",
            "|\" U.S. stock...|Business|[0.999415625...|  0.0|       0.0|\n",
            "|\" U.S. blue ...|Business|[0.997383744...|  0.0|       0.0|\n",
            "|Attorney Gen...|Business|[0.996676286...|  0.0|       0.0|\n",
            "|\" Stocks fel...|Business|[0.995670023...|  0.0|       0.0|\n",
            "|The airline ...|Business|[0.994536408...|  0.0|       0.0|\n",
            "|\" Shares of ...|Business|[0.993787221...|  0.0|       0.0|\n",
            "|\" Stocks sli...|Business|[0.993759879...|  0.0|       0.0|\n",
            "|\" Mid priced...|Business|[0.993214287...|  0.0|       0.0|\n",
            "|\" Citigroup ...|Business|[0.993153775...|  0.0|       0.0|\n",
            "| OPEC oil pr...|Business|[0.993096689...|  0.0|       0.0|\n",
            "+---------------+--------+---------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "\n",
        "evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnkM8NQonLs6",
        "outputId": "f7cfcbae-b530-4f6c-f88e-235433da3fc7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.901318208334548"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "y_true = predictions.select(\"label\")\n",
        "y_true = y_true.toPandas()\n",
        "\n",
        "y_pred = predictions.select(\"prediction\")\n",
        "y_pred = y_pred.toPandas()"
      ],
      "metadata": {
        "id": "AfpwbRiToidV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred.prediction.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "GpLHrjupowib",
        "outputId": "85409733-84de-47ec-aa79-02996183acc6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "prediction\n",
              "2.0    9374\n",
              "1.0    9077\n",
              "0.0    9017\n",
              "3.0    8529\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>prediction</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <td>9374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>9077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>9017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.0</th>\n",
              "      <td>8529</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnf_matrix = confusion_matrix(list(y_true.label.astype(int)), list(y_pred.prediction.astype(int)))\n",
        "cnf_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V_HYdPIpZK-",
        "outputId": "0c81bf2c-f13a-4809-d790-8a01abd73071"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7797,  789,  104,  287],\n",
              "       [ 699, 7899,   86,  299],\n",
              "       [  47,   77, 8902,   90],\n",
              "       [ 474,  312,  282, 7853]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true.label, y_pred.prediction))\n",
        "print(accuracy_score(y_true.label, y_pred.prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FGJPVonpgwC",
        "outputId": "78208396-9101-4b54-c417-8212419b8789"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.87      0.87      8977\n",
            "         1.0       0.87      0.88      0.87      8983\n",
            "         2.0       0.95      0.98      0.96      9116\n",
            "         3.0       0.92      0.88      0.90      8921\n",
            "\n",
            "    accuracy                           0.90     35997\n",
            "   macro avg       0.90      0.90      0.90     35997\n",
            "weighted avg       0.90      0.90      0.90     35997\n",
            "\n",
            "0.9014917909825819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LogReg with TFIDF\n"
      ],
      "metadata": {
        "id": "JcTrfpwuplmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"token_features\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "\n",
        "nlp_pipeline_tf = Pipeline(\n",
        "    stages=[document_assembler,\n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner,\n",
        "            stemmer,\n",
        "            finisher,\n",
        "            hashingTF,\n",
        "            idf,\n",
        "            label_stringIdx])\n",
        "\n",
        "nlp_model_tf = nlp_pipeline_tf.fit(newsDF)\n",
        "\n",
        "processed_tf = nlp_model_tf.transform(newsDF)\n",
        "\n",
        "processed_tf.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47TDYElmpqXI",
        "outputId": "c3a254f0-e0a3-4e84-de50-41e24efb374d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120000"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed for reproducibility\n",
        "processed_tf.select('description','features','label').show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb1bXe38qGwV",
        "outputId": "65434b5c-2e32-4d66-d50d-d55ceb7736da"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-----+\n",
            "|         description|            features|label|\n",
            "+--------------------+--------------------+-----+\n",
            "| Short sellers, W...|(10000,[551,621,6...|  0.0|\n",
            "| Private investme...|(10000,[157,831,9...|  0.0|\n",
            "| Soaring crude pr...|(10000,[793,1738,...|  0.0|\n",
            "| Authorities have...|(10000,[1548,1611...|  0.0|\n",
            "| Tearaway world o...|(10000,[323,585,1...|  0.0|\n",
            "+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(trainingData, testData) = processed_tf.randomSplit([0.7, 0.3], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT_wlSWlqQES",
        "outputId": "0d439b2a-683f-492b-a67e-9cf407b4a973"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Count: 84003\n",
            "Test Dataset Count: 35997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lrModel_tf = lr.fit(trainingData)\n",
        "\n",
        "predictions_tf = lrModel_tf.transform(testData)\n",
        "\n",
        "predictions_tf.select(\"description\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4F78FWKqXvH",
        "outputId": "2b1611c5-cf84-49e0-95d0-bd4c755508cd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------+---------------+-----+----------+\n",
            "|    description|category|    probability|label|prediction|\n",
            "+---------------+--------+---------------+-----+----------+\n",
            "|\" U.S. stock...|Business|[0.998061427...|  0.0|       0.0|\n",
            "|Attorney Gen...|Business|[0.995454968...|  0.0|       0.0|\n",
            "|\" Stocks fel...|Business|[0.995193741...|  0.0|       0.0|\n",
            "|\" U.S. regul...|Business|[0.994795976...|  0.0|       0.0|\n",
            "|Former Enron...|Business|[0.993438909...|  0.0|       0.0|\n",
            "|\" Mid priced...|Business|[0.993402129...|  0.0|       0.0|\n",
            "|\" Stocks sli...|Business|[0.992725234...|  0.0|       0.0|\n",
            "|In NEW YORK,...|Business|[0.992287637...|  0.0|       0.0|\n",
            "| Interest ra...|Business|[0.991581683...|  0.0|       0.0|\n",
            "|\" Shares of ...|Business|[0.991295981...|  0.0|       0.0|\n",
            "+---------------+--------+---------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest with TFIDF"
      ],
      "metadata": {
        "id": "Euq5AWhGtQAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
        "                            featuresCol=\"features\", \\\n",
        "                            numTrees = 100, \\\n",
        "                            maxDepth = 4, \\\n",
        "                            maxBins = 32)\n",
        "\n",
        "# train model with Training Data\n",
        "rfModel = rf.fit(trainingData)\n",
        "predictions_rf = rfModel.transform(testData)"
      ],
      "metadata": {
        "id": "yIOPrFNXtWAD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_rf.select(\"description\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9Ye5m31ukQJ",
        "outputId": "aa2e58fc-bd34-436a-8282-4851f155667f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------+---------------+-----+----------+\n",
            "|    description|category|    probability|label|prediction|\n",
            "+---------------+--------+---------------+-----+----------+\n",
            "|\" Stocks fel...|Business|[0.407714917...|  0.0|       0.0|\n",
            "|Shares of Ma...|Business|[0.399823021...|  0.0|       0.0|\n",
            "|end jewelry ...|Business|[0.397368065...|  0.0|       0.0|\n",
            "|\" Amazon.com...|Business|[0.388693784...|  0.0|       0.0|\n",
            "|Shares of Me...|Business|[0.387333747...|  0.0|       0.0|\n",
            "|Amazon.com I...|Business|[0.384674689...|  0.0|       0.0|\n",
            "|Shares of dr...|Business|[0.383754243...|  0.0|       0.0|\n",
            "|US investmen...|Business|[0.381760862...|  0.0|       0.0|\n",
            "|Shares of Ha...|Business|[0.379005165...|  0.0|       0.0|\n",
            "| Oil prices ...|Business|[0.378993627...|  0.0|       0.0|\n",
            "+---------------+--------+---------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = predictions_rf.select(\"label\")\n",
        "y_true = y_true.toPandas()\n",
        "\n",
        "y_pred = predictions_rf.select(\"prediction\")\n",
        "y_pred = y_pred.toPandas()\n",
        "\n",
        "print(classification_report(y_true.label, y_pred.prediction))\n",
        "print(accuracy_score(y_true.label, y_pred.prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGADuJoNvKl8",
        "outputId": "3bf7f512-1a44-4884-d180-7175ea571c50"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.69      0.73      8977\n",
            "         1.0       0.77      0.61      0.68      8983\n",
            "         2.0       0.82      0.83      0.82      9116\n",
            "         3.0       0.65      0.84      0.73      8921\n",
            "\n",
            "    accuracy                           0.74     35997\n",
            "   macro avg       0.75      0.74      0.74     35997\n",
            "weighted avg       0.75      0.74      0.74     35997\n",
            "\n",
            "0.7427007806206073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LogReg with Spark NLP Glove Word Embeddings"
      ],
      "metadata": {
        "id": "S1-AgfPyvNBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_assembler = DocumentAssembler() \\\n",
        "      .setInputCol(\"description\") \\\n",
        "      .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "      .setInputCols([\"document\"]) \\\n",
        "      .setOutputCol(\"token\")\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "      .setInputCols([\"token\"]) \\\n",
        "      .setOutputCol(\"normalized\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"normalized\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\n",
        "\n",
        "glove_embeddings = WordEmbeddingsModel().pretrained() \\\n",
        "      .setInputCols([\"document\",'cleanTokens'])\\\n",
        "      .setOutputCol(\"embeddings\")\\\n",
        "      .setCaseSensitive(False)\n",
        "\n",
        "embeddingsSentence = SentenceEmbeddings() \\\n",
        "      .setInputCols([\"document\", \"embeddings\"]) \\\n",
        "      .setOutputCol(\"sentence_embeddings\") \\\n",
        "      .setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "embeddings_finisher = EmbeddingsFinisher() \\\n",
        "      .setInputCols([\"sentence_embeddings\"]) \\\n",
        "      .setOutputCols([\"finished_sentence_embeddings\"]) \\\n",
        "      .setOutputAsVector(True)\\\n",
        "      .setCleanAnnotations(False)\n",
        "\n",
        "explodeVectors = SQLTransformer(statement=\n",
        "      \"SELECT EXPLODE(finished_sentence_embeddings) AS features, * FROM __THIS__\") # explodying array column in DataFrame in mutliple rows while retainin the other columns\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
        "\n",
        "\n",
        "nlp_pipeline_w2v = Pipeline(\n",
        "    stages=[document_assembler,\n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner,\n",
        "            glove_embeddings,\n",
        "            embeddingsSentence,\n",
        "            embeddings_finisher,\n",
        "            explodeVectors,\n",
        "            label_stringIdx])\n",
        "\n",
        "nlp_model_w2v = nlp_pipeline_w2v.fit(newsDF)\n",
        "\n",
        "processed_w2v = nlp_model_w2v.transform(newsDF)\n",
        "\n",
        "processed_w2v.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me7h5YZ4wtNu",
        "outputId": "862e7f49-3197-4f1d-f35c-b64928520ec2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120000"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_w2v.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzLvRpUuzO5-",
        "outputId": "ca9e9a64-cb0a-4d97-f3bd-dd1e00af8f75"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['features',\n",
              " 'category',\n",
              " 'description',\n",
              " 'document',\n",
              " 'token',\n",
              " 'normalized',\n",
              " 'cleanTokens',\n",
              " 'embeddings',\n",
              " 'sentence_embeddings',\n",
              " 'finished_sentence_embeddings',\n",
              " 'label']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_w2v.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4g-aSzMzQrJ",
        "outputId": "120fa080-ab28-4e87-f499-f1044e90f94a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----+\n",
            "|            features|category|         description|            document|               token|          normalized|         cleanTokens|          embeddings| sentence_embeddings|finished_sentence_embeddings|label|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----+\n",
            "|[-0.1556767076253...|Business| Short sellers, W...|[{document, 0, 84...|[{token, 1, 5, Sh...|[{token, 1, 5, Sh...|[{token, 1, 5, Sh...|[{word_embeddings...|[{sentence_embedd...|        [[-0.155676707625...|  0.0|\n",
            "|[-0.0144653050228...|Business| Private investme...|[{document, 0, 20...|[{token, 1, 7, Pr...|[{token, 1, 7, Pr...|[{token, 1, 7, Pr...|[{word_embeddings...|[{sentence_embedd...|        [[-0.014465305022...|  0.0|\n",
            "|[0.10348732769489...|Business| Soaring crude pr...|[{document, 0, 17...|[{token, 1, 7, So...|[{token, 1, 7, So...|[{token, 1, 7, So...|[{word_embeddings...|[{sentence_embedd...|        [[0.1034873276948...|  0.0|\n",
            "|[-0.0355810523033...|Business| Authorities have...|[{document, 0, 18...|[{token, 1, 11, A...|[{token, 1, 11, A...|[{token, 1, 11, A...|[{word_embeddings...|[{sentence_embedd...|        [[-0.035581052303...|  0.0|\n",
            "|[0.00647281948477...|Business| Tearaway world o...|[{document, 0, 15...|[{token, 1, 8, Te...|[{token, 1, 8, Te...|[{token, 1, 8, Te...|[{word_embeddings...|[{sentence_embedd...|        [[0.0064728194847...|  0.0|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_w2v.select('finished_sentence_embeddings').take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfc0vsKMzTFn",
        "outputId": "d73c55d2-e320-4f77-abb1-9cc77857306f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(finished_sentence_embeddings=[DenseVector([-0.1557, 0.196, 0.1099, -0.3089, 0.16, 0.1672, -0.4649, -0.1101, -0.053, -0.1551, 0.0327, 0.0772, 0.1494, -0.1865, 0.1155, -0.0597, 0.0234, -0.0451, 0.2361, -0.0089, 0.3358, 0.0444, 0.0088, -0.1453, 0.2289, 0.0914, -0.1665, -0.3726, 0.1892, 0.121, 0.1993, -0.0239, -0.1346, 0.1159, 0.2086, 0.1285, 0.068, 0.1372, 0.3153, -0.1934, 0.0257, -0.226, -0.0984, 0.1139, 0.1413, -0.3743, 0.072, 0.1403, 0.251, -0.3106, 0.1709, -0.0697, -0.0554, 0.5123, -0.1873, -1.7784, 0.0295, 0.1014, 0.9268, 0.2129, -0.1354, 0.5739, -0.0679, 0.461, 0.4216, 0.0225, 0.4456, -0.2462, 0.1411, -0.3258, 0.0025, 0.0114, -0.3895, -0.1106, -0.261, 0.0147, 0.0781, 0.1268, -0.2042, -0.2278, 0.5096, 0.1539, -0.3515, -0.0102, -0.7003, -0.3872, -0.1668, -0.2405, -0.0766, 0.1396, -0.0592, -0.1568, -0.1606, -0.1371, -0.684, -0.2549, -0.1541, 0.1536, 0.2715, 0.3342])])]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "\n",
        "# processed_w2v= processed_w2v.withColumn(\"features\", explode(processed_w2v.finished_sentence_embeddings))"
      ],
      "metadata": {
        "id": "-jZNeHyNzbEc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_w2v.select(\"features\").take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVcKzcG3zeM0",
        "outputId": "f1b0d27d-2b7f-415d-d658-76eb67556aa3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(features=DenseVector([-0.1557, 0.196, 0.1099, -0.3089, 0.16, 0.1672, -0.4649, -0.1101, -0.053, -0.1551, 0.0327, 0.0772, 0.1494, -0.1865, 0.1155, -0.0597, 0.0234, -0.0451, 0.2361, -0.0089, 0.3358, 0.0444, 0.0088, -0.1453, 0.2289, 0.0914, -0.1665, -0.3726, 0.1892, 0.121, 0.1993, -0.0239, -0.1346, 0.1159, 0.2086, 0.1285, 0.068, 0.1372, 0.3153, -0.1934, 0.0257, -0.226, -0.0984, 0.1139, 0.1413, -0.3743, 0.072, 0.1403, 0.251, -0.3106, 0.1709, -0.0697, -0.0554, 0.5123, -0.1873, -1.7784, 0.0295, 0.1014, 0.9268, 0.2129, -0.1354, 0.5739, -0.0679, 0.461, 0.4216, 0.0225, 0.4456, -0.2462, 0.1411, -0.3258, 0.0025, 0.0114, -0.3895, -0.1106, -0.261, 0.0147, 0.0781, 0.1268, -0.2042, -0.2278, 0.5096, 0.1539, -0.3515, -0.0102, -0.7003, -0.3872, -0.1668, -0.2405, -0.0766, 0.1396, -0.0592, -0.1568, -0.1606, -0.1371, -0.684, -0.2549, -0.1541, 0.1536, 0.2715, 0.3342]))]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_w2v.select(\"features\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBAO4emfzirv",
        "outputId": "b35bc097-a499-4009-a338-101c993dcd7c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|            features|\n",
            "+--------------------+\n",
            "|[-0.1556767076253...|\n",
            "|[-0.0144653050228...|\n",
            "|[0.10348732769489...|\n",
            "|[-0.0355810523033...|\n",
            "|[0.00647281948477...|\n",
            "+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_w2v.select('description','features','label').show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y3unYcTzlFC",
        "outputId": "b379518b-bd70-4060-da0a-3e48237930eb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-----+\n",
            "|         description|            features|label|\n",
            "+--------------------+--------------------+-----+\n",
            "| Short sellers, W...|[-0.1556767076253...|  0.0|\n",
            "| Private investme...|[-0.0144653050228...|  0.0|\n",
            "| Soaring crude pr...|[0.10348732769489...|  0.0|\n",
            "| Authorities have...|[-0.0355810523033...|  0.0|\n",
            "| Tearaway world o...|[0.00647281948477...|  0.0|\n",
            "+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed for reproducibility\n",
        "(trainingData, testData) = processed_w2v.randomSplit([0.7, 0.3], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftfHo5zEzp-t",
        "outputId": "a6c0ce11-2c73-49c1-e6ba-a3b58df3cb48"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Count: 84003\n",
            "Test Dataset Count: 35997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "\n",
        "# filter rows in a DataFrame based on the number of non-zero elements in a features column.\n",
        "\n",
        "@udf(\"long\")\n",
        "def num_nonzeros(v):\n",
        "    return v.numNonzeros()\n",
        "# Filter out rows with zero-dimension features in both trainingData and testData\n",
        "trainingData = trainingData.where(num_nonzeros(\"features\") != 0)\n",
        "testData = testData.where(num_nonzeros(\"features\") != 0)\n",
        "\n",
        "# Alternatively, you can filter based on the size of finished_sentence_embeddings\n",
        "# trainingData = trainingData.filter(size(trainingData[\"finished_sentence_embeddings\"]) > 0)\n",
        "# testData = testData.filter(size(testData[\"finished_sentence_embeddings\"]) > 0)\n",
        "\n",
        "# Another option: replace zero vectors with a vector of ones with the correct dimension\n",
        "# First, get the dimension of the feature vectors\n",
        "feature_dim = len(trainingData.select(\"features\").first()[0])\n",
        "\n",
        "# Define a UDF to replace zero vectors\n",
        "@udf(\"array<double>\")\n",
        "def replace_zero_vector(v):\n",
        "    if v.numNonzeros() == 0:\n",
        "        return [1.0] * feature_dim  # Replace with a vector of ones\n",
        "    else:\n",
        "        return v.toArray().tolist()\n",
        "# Apply the UDF to the features column\n",
        "trainingData = trainingData.withColumn(\"features\", replace_zero_vector(\"features\"))\n",
        "testData = testData.withColumn(\"features\", replace_zero_vector(\"features\"))\n"
      ],
      "metadata": {
        "id": "W8bSH9gV1U3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "# Define a UDF to convert the array<double> to VectorUDT\n",
        "@udf(returnType=VectorUDT())\n",
        "def array_to_vector(features):\n",
        "    return Vectors.dense(features)\n",
        "\n",
        "# Apply the UDF to the 'features' column of both trainingData and testData\n",
        "trainingData = trainingData.withColumn(\"features\", array_to_vector(\"features\"))\n",
        "testData = testData.withColumn(\"features\", array_to_vector(\"features\"))\n",
        "\n",
        "# Now you can fit the model\n",
        "lrModel_w2v = lr.fit(trainingData)"
      ],
      "metadata": {
        "id": "jYGeMPzq41b5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_w2v = lrModel_w2v.transform(testData)\n",
        "\n",
        "predictions_w2v.select(\"description\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 15)"
      ],
      "metadata": {
        "id": "FD7VLW_14-tk",
        "outputId": "cb405647-0fb1-4de3-85cb-f25fd655c65e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------+---------------+-----+----------+\n",
            "|    description|category|    probability|label|prediction|\n",
            "+---------------+--------+---------------+-----+----------+\n",
            "|THE stock ma...|Business|[0.989105694...|  0.0|       0.0|\n",
            "|Wachovia Cor...|Business|[0.988506150...|  0.0|       0.0|\n",
            "| Stocks fell...|Business|[0.985367050...|  0.0|       0.0|\n",
            "|Japan #39;s ...|Business|[0.983635621...|  0.0|       0.0|\n",
            "| ChevronTexa...|Business|[0.981973089...|  0.0|       0.0|\n",
            "| US investme...|Business|[0.981749547...|  0.0|       0.0|\n",
            "|  Shares of ...|Business|[0.981273669...|  0.0|       0.0|\n",
            "| Goldman Sac...|Business|[0.981270793...|  0.0|       0.0|\n",
            "| Tokyo stock...|Business|[0.980858489...|  0.0|       0.0|\n",
            "|British Airw...|Business|[0.979088218...|  0.0|       0.0|\n",
            "+---------------+--------+---------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "y_true = predictions_w2v.select(\"label\")\n",
        "y_true = y_true.toPandas()\n",
        "\n",
        "y_pred = predictions_w2v.select(\"prediction\")\n",
        "y_pred = y_pred.toPandas()\n",
        "\n",
        "print(classification_report(y_true.label, y_pred.prediction))\n",
        "print(accuracy_score(y_true.label, y_pred.prediction))"
      ],
      "metadata": {
        "id": "oCx5b4Xg6okx",
        "outputId": "c014ef45-756e-440f-a967-f68a69968acc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.82      0.82      9051\n",
            "         1.0       0.82      0.81      0.82      9057\n",
            "         2.0       0.93      0.96      0.94      8972\n",
            "         3.0       0.88      0.87      0.87      8917\n",
            "\n",
            "    accuracy                           0.86     35997\n",
            "   macro avg       0.86      0.86      0.86     35997\n",
            "weighted avg       0.86      0.86      0.86     35997\n",
            "\n",
            "0.8643775870211406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_w2v.select('description','cleanTokens.result').show(truncate=15)"
      ],
      "metadata": {
        "id": "dLA-8YAr6uzo",
        "outputId": "80e0003c-482d-41ff-9abd-f5959d19b8a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------+\n",
            "|    description|         result|\n",
            "+---------------+---------------+\n",
            "| Short selle...|[Short, sell...|\n",
            "| Private inv...|[Private, in...|\n",
            "| Soaring cru...|[Soaring, cr...|\n",
            "| Authorities...|[Authorities...|\n",
            "| Tearaway wo...|[Tearaway, w...|\n",
            "| Stocks ende...|[Stocks, end...|\n",
            "| Assets of t...|[Assets, nat...|\n",
            "| Retail sale...|[Retail, sal...|\n",
            "|\" After earn...|[earning, PH...|\n",
            "| Short selle...|[Short, sell...|\n",
            "| Soaring cru...|[Soaring, cr...|\n",
            "| OPEC can do...|[OPEC, nothi...|\n",
            "| Non OPEC oi...|[Non, OPEC, ...|\n",
            "| WASHINGTON/...|[WASHINGTONN...|\n",
            "| The dollar ...|[dollar, tum...|\n",
            "|If you think...|[think, may,...|\n",
            "|The purchasi...|[purchasing,...|\n",
            "|There is lit...|[little, cau...|\n",
            "|The US trade...|[US, trade, ...|\n",
            "|Oil giant Sh...|[Oil, giant,...|\n",
            "+---------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LogReg with Spark NLP Bert Embeddings"
      ],
      "metadata": {
        "id": "SZ3t8Co1_XRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_assembler = DocumentAssembler() \\\n",
        "      .setInputCol(\"description\") \\\n",
        "      .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "      .setInputCols([\"document\"]) \\\n",
        "      .setOutputCol(\"token\")\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "      .setInputCols([\"token\"]) \\\n",
        "      .setOutputCol(\"normalized\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"normalized\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\n",
        "\n",
        "bert_embeddings = BertEmbeddings.pretrained('bert_base_cased', 'en') \\\n",
        "      .setInputCols([\"document\",'cleanTokens'])\\\n",
        "      .setOutputCol(\"bert\")\\\n",
        "      .setCaseSensitive(False)\\\n",
        "\n",
        "embeddingsSentence = SentenceEmbeddings() \\\n",
        "      .setInputCols([\"document\", \"bert\"]) \\\n",
        "      .setOutputCol(\"sentence_embeddings\") \\\n",
        "      .setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "embeddings_finisher = EmbeddingsFinisher() \\\n",
        "      .setInputCols([\"sentence_embeddings\"]) \\\n",
        "      .setOutputCols([\"finished_sentence_embeddings\"]) \\\n",
        "      .setOutputAsVector(True)\\\n",
        "      .setCleanAnnotations(False)\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
        "\n",
        "\n",
        "nlp_pipeline_bert = Pipeline(\n",
        "    stages=[document_assembler,\n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner,\n",
        "            bert_embeddings,\n",
        "            embeddingsSentence,\n",
        "            embeddings_finisher,\n",
        "            label_stringIdx])\n"
      ],
      "metadata": {
        "id": "kM15iZ_N_U3o",
        "outputId": "fef4a55b-8363-4154-d9f9-e3b59de26fb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 384.9 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "limited_df = newsDF.limit(1000)\n",
        "\n",
        "nlp_model_bert = nlp_pipeline_bert.fit(limited_df)\n",
        "\n",
        "processed_bert = nlp_model_bert.transform(limited_df)\n",
        "\n",
        "processed_bert.count()"
      ],
      "metadata": {
        "id": "xvEpxLDn_h7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99d514a-688e-478e-aeac-3ab254adbdd3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.68 s, sys: 303 ms, total: 2.98 s\n",
            "Wall time: 14min 31s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "\n",
        "processed_bert = processed_bert.withColumn(\"features\", explode(processed_bert.finished_sentence_embeddings))\n",
        "\n",
        "processed_bert.select('description','features','label').show()"
      ],
      "metadata": {
        "id": "7NXsTgkk_ma6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1d62a4-3638-45a2-84a7-3bfa1d72e58b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-----+\n",
            "|         description|            features|label|\n",
            "+--------------------+--------------------+-----+\n",
            "| Short sellers, W...|[-0.0012149482499...|  2.0|\n",
            "| Private investme...|[0.13144019246101...|  2.0|\n",
            "| Soaring crude pr...|[-0.1905521601438...|  2.0|\n",
            "| Authorities have...|[0.06882479041814...|  2.0|\n",
            "| Tearaway world o...|[-0.1174716278910...|  2.0|\n",
            "| Stocks ended sli...|[-0.0321817845106...|  2.0|\n",
            "| Assets of the na...|[-0.2906664013862...|  2.0|\n",
            "| Retail sales bou...|[-0.0385283492505...|  2.0|\n",
            "|\" After earning a...|[-0.0362812504172...|  2.0|\n",
            "| Short sellers, W...|[-0.0012149482499...|  2.0|\n",
            "| Soaring crude pr...|[-0.1905521601438...|  2.0|\n",
            "| OPEC can do noth...|[-0.1431127935647...|  2.0|\n",
            "| Non OPEC oil exp...|[0.01600192859768...|  2.0|\n",
            "| WASHINGTON/NEW Y...|[0.14494347572326...|  2.0|\n",
            "| The dollar tumbl...|[-0.1958881020545...|  2.0|\n",
            "|If you think you ...|[0.27292791008949...|  2.0|\n",
            "|The purchasing po...|[0.00386757543310...|  2.0|\n",
            "|There is little c...|[0.10686700046062...|  2.0|\n",
            "|The US trade defi...|[-0.1425888091325...|  2.0|\n",
            "|Oil giant Shell c...|[0.14418697357177...|  2.0|\n",
            "+--------------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed for reproducibility\n",
        "(trainingData, testData) = processed_bert.randomSplit([0.7, 0.3], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ],
      "metadata": {
        "id": "lWXcuTg9_o8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90bb9e19-a3ae-4b8d-cc60-4f0d9a73a7b8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Count: 703\n",
            "Test Dataset Count: 297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "\n",
        "lrModel = lr.fit(trainingData)"
      ],
      "metadata": {
        "id": "oBAGKKcs_wJm"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "\n",
        "@udf(\"long\")\n",
        "def num_nonzeros(v):\n",
        "    return v.numNonzeros()\n",
        "\n",
        "testData = testData.where(num_nonzeros(\"features\") != 0)"
      ],
      "metadata": {
        "id": "_86c1L_B_3qV"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = lrModel.transform(testData)\n",
        "\n",
        "predictions.select(\"description\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 15)"
      ],
      "metadata": {
        "id": "6-7sZxlK_9Gr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4abeb6b3-77c9-4ed6-e55a-2cd480ebde93"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------+---------------+-----+----------+\n",
            "|    description|category|    probability|label|prediction|\n",
            "+---------------+--------+---------------+-----+----------+\n",
            "|Prototype 90...|Sci/Tech|[0.997235280...|  0.0|       0.0|\n",
            "|  In this un...|Sci/Tech|[0.996880983...|  0.0|       0.0|\n",
            "| Gamma ray b...|Sci/Tech|[0.995862429...|  0.0|       0.0|\n",
            "|At the outer...|Sci/Tech|[0.995525995...|  0.0|       0.0|\n",
            "| Designed to...|Sci/Tech|[0.994857232...|  0.0|       0.0|\n",
            "|The jury's s...|Business|[0.994484828...|  2.0|       0.0|\n",
            "|It's ironic ...|Sci/Tech|[0.993564951...|  0.0|       0.0|\n",
            "|\\\\For some r...|Sci/Tech|[0.993047116...|  0.0|       0.0|\n",
            "|A possible T...|Sci/Tech|[0.992974608...|  0.0|       0.0|\n",
            "|Research In ...|Sci/Tech|[0.992067154...|  0.0|       0.0|\n",
            "+---------------+--------+---------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "df = predictions.select('description','category','label','prediction').toPandas()\n",
        "\n",
        "print(classification_report(df.label, df.prediction))\n",
        "print(accuracy_score(df.label, df.prediction))"
      ],
      "metadata": {
        "id": "RDl-Sa8SAFv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d23395c-80e1-487e-d1e0-d7423b260649"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.89      0.84       133\n",
            "         1.0       0.84      0.76      0.80        71\n",
            "         2.0       0.66      0.55      0.60        53\n",
            "         3.0       0.86      0.90      0.88        40\n",
            "\n",
            "    accuracy                           0.80       297\n",
            "   macro avg       0.79      0.77      0.78       297\n",
            "weighted avg       0.79      0.80      0.79       297\n",
            "\n",
            "0.797979797979798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html \"/content/Text_Classification_with_Spark_NLP.ipynb\""
      ],
      "metadata": {
        "id": "PmX2jjHHAPgR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd985305-ace6-4067-963c-2f719b482c7d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/Text_Classification_with_Spark_NLP.ipynb to html\n",
            "[NbConvertApp] Writing 395197 bytes to /content/Text_Classification_with_Spark_NLP.html\n"
          ]
        }
      ]
    }
  ]
}